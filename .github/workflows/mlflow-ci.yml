name: CI Pipeline - MLflow Boston Housing

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]
  workflow_dispatch:

env:
  MLFLOW_TRACKING_URI: https://dagshub.com/rezahmas/boston-housing-mlflow.mlflow
  MLFLOW_TRACKING_USERNAME: rezahmas
  MLFLOW_TRACKING_PASSWORD: ${{ secrets.DAGSHUB_TOKEN }}
  MLFLOW_EXPERIMENT_NAME: CI_Docker_Build
  DOCKER_USER: ${{ secrets.DOCKER_USERNAME }}
  IMAGE_NAME: boston-model
  
  # =========================================================
  # FIX UTAMA: KREDENSIAL S3 (Wajib untuk upload DagsHub)
  # =========================================================
  # DagsHub menggunakan Token sebagai Access Key & Secret Key
  AWS_ACCESS_KEY_ID: ${{ secrets.DAGSHUB_TOKEN }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.DAGSHUB_TOKEN }}
  AWS_REGION: us-east-1
  # Endpoint khusus storage DagsHub
  MLFLOW_S3_ENDPOINT_URL: https://dagshub.com/api/v1/repo/rezahmas/boston-housing-mlflow/storage

jobs:
  build-train-docker:
    runs-on: ubuntu-latest
    
    steps:
      # ==================== 1. SETUP ====================
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      # ==================== 2. INSTALL ====================
      - name: üì¶ Install dependencies
        run: |
          pip install --upgrade pip
          if [ -f "Workflow-CI/MLProject/requirements.txt" ]; then
            pip install -r Workflow-CI/MLProject/requirements.txt
          fi
          # Wajib boto3 untuk S3 upload
          pip install pandas requests boto3 dagshub mlflow scikit-learn

      # ==================== 3. VERIFY DATA ====================
      - name: ‚úÖ Verify clean dataset exists
        run: |
          if [ ! -f "Workflow-CI/MLProject/HousingData_clean.csv" ]; then
            echo "‚ùå ERROR: HousingData_clean.csv tidak ditemukan!"
            exit 1
          fi
          echo "‚úì Dataset found."

      # ==================== 4. TRAINING ====================
      - name: üöÄ Train models & Upload
        run: |
          cd Workflow-CI/MLProject
          # Kita set environment variable khusus di step ini agar MLflow sadar S3
          export MLFLOW_S3_ENDPOINT_URL="https://dagshub.com/api/v1/repo/rezahmas/boston-housing-mlflow/storage"
          python modelling.py

      # ==================== 5. VERIFY ARTIFACT ====================
      - name: üîç Verify 'model' artifact
        id: get_run_id
        run: |
          cd Workflow-CI/MLProject
          python3 << 'EOF'
          import mlflow
          from mlflow.tracking import MlflowClient
          import os
          import sys
          import time

          # Setup
          mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
          client = MlflowClient()
          exp_name = os.environ['MLFLOW_EXPERIMENT_NAME']
          
          print(f"üîç Checking Experiment: {exp_name}")
          experiment = mlflow.get_experiment_by_name(exp_name)
          
          if not experiment:
              print("‚ùå Experiment not found")
              sys.exit(1)
          
          # Tunggu sebentar (kadang latency S3)
          time.sleep(5) 

          # Cari Run Terakhir
          runs = mlflow.search_runs(
              [experiment.experiment_id], 
              filter_string="status='FINISHED'", 
              order_by=["start_time DESC"], 
              max_results=1
          )
          
          if runs.empty:
              print("‚ùå No FINISHED runs found")
              sys.exit(1)
              
          run_id = runs.iloc[0].run_id
          print(f"üÜî Run ID: {run_id}")
          
          # Cek Isi Artefak
          artifacts = client.list_artifacts(run_id)
          paths = [a.path for a in artifacts]
          print(f"üìÇ Artifacts found: {paths}")
          
          if "model" not in paths:
              print(f"‚ùå CRITICAL: 'model' folder missing!")
              print("   Cek apakah AWS_ACCESS_KEY_ID ter-set di secrets.")
              sys.exit(1)
              
          print("‚úÖ Artifact verified.")
          
          with open(os.environ['GITHUB_ENV'], 'a') as f:
              f.write(f"RUN_ID={run_id}\n")
          EOF

      # ==================== 6. BUILD DOCKER ====================
      - name: üê≥ Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: üèóÔ∏è Build Docker image
        run: |
          echo "Building from Run ID: ${{ env.RUN_ID }}"
          mlflow models build-docker \
            --model-uri "runs:/${{ env.RUN_ID }}/model" \
            --name "${{ env.DOCKER_USER }}/${{ env.IMAGE_NAME }}:latest"

      # ==================== 7. TEST CONTAINER ====================
      - name: üß™ Test Docker container
        run: |
          docker run -d -p 8080:8080 --name test-container ${{ env.DOCKER_USER }}/${{ env.IMAGE_NAME }}:latest
          sleep 20
          
          if ! curl -f http://localhost:8080/ping; then
             docker logs test-container
             exit 1
          fi
          
          RESPONSE=$(curl -X POST http://localhost:8080/invocations \
            -H "Content-Type: application/json" \
            -d '{"dataframe_split": {"columns": ["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","B","LSTAT"], "data": [[0.00632,18.0,2.31,0.0,0.538,6.575,65.2,4.09,1.0,296.0,15.3,396.9,4.98]]}}')
          
          if [[ "$RESPONSE" == *"["* ]]; then
             echo "‚úì Prediction success"
          else
             exit 1
          fi
          docker rm -f test-container

      # ==================== 8. PUSH ====================
      - name: üîê Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: üì§ Push Docker image
        run: docker push ${{ env.DOCKER_USER }}/${{ env.IMAGE_NAME }}:latest